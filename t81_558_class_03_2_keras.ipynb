{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_03_2_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 3: Introduction to TensorFlow**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Material\n",
    "\n",
    "* Part 3.1: Deep Learning and Neural Network Introduction [[Video]](https://www.youtube.com/watch?v=zYnI4iWRmpc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_1_neural_net.ipynb)\n",
    "* **Part 3.2: Introduction to Tensorflow and Keras** [[Video]](https://www.youtube.com/watch?v=PsE73jk55cE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_2_keras.ipynb)\n",
    "* Part 3.3: Saving and Loading a Keras Neural Network [[Video]](https://www.youtube.com/watch?v=-9QfbGM1qGw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_3_save_load.ipynb)\n",
    "* Part 3.4: Early Stopping in Keras to Prevent Overfitting [[Video]](https://www.youtube.com/watch?v=m1LNunuI2fk&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_4_early_stop.ipynb)\n",
    "* Part 3.5: Extracting Weights and Manual Calculation [[Video]](https://www.youtube.com/watch?v=7PWgx16kH8s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_5_weights.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.2: Introduction to Tensorflow and Keras\n",
    "\n",
    "TensorFlow is an open-source software library for machine learning in various kinds of perceptual and language understanding tasks. It is currently used for both research and production by different teams in many commercial Google products, such as speech recognition, Gmail, Google Photos, and search, many of which had previously used its predecessor DistBelief. TensorFlow was originally developed by the Google Brain team for Google's research and production purposes and later released under the Apache 2.0 open source license on November 9, 2015.\n",
    "\n",
    "* [TensorFlow Homepage](https://www.tensorflow.org/)\n",
    "* [TensorFlow GitHib](https://github.com/tensorflow/tensorflow)\n",
    "* [TensorFlow Google Groups Support](https://groups.google.com/forum/#!forum/tensorflow)\n",
    "* [TensorFlow Google Groups Developer Discussion](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\n",
    "* [TensorFlow FAQ](https://www.tensorflow.org/resources/faq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why TensorFlow\n",
    "\n",
    "* Supported by Google\n",
    "* Works well on Windows, Linux, and Mac\n",
    "* Excellent GPU support\n",
    "* Python is an easy to learn programming language\n",
    "* Python is extremely popular in the data science community\n",
    "\n",
    "## Deep Learning Tools\n",
    "TensorFlow is not the only game in town. The biggest competitor to TensorFlow/Keras is PyTorch. Listed below are some of the deep learning toolkits actively being supported:\n",
    "\n",
    "* **[TensorFlow](https://www.tensorflow.org/)** - Google's deep learning API.  The focus of this class, along with Keras.\n",
    "* **[Keras](https://keras.io/)** - Also by Google, higher level framework that allows the use of TensorFlow, MXNet and Theano interchangeably.\n",
    "* **[PyTorch](https://pytorch.org/)** - PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's AI Research lab. \n",
    "\n",
    "Other deep learning tools:\n",
    "\n",
    "* **[MXNet](https://mxnet.incubator.apache.org/)** Apache foundation's deep learning API. Can be used through Keras.\n",
    "* **[Torch](http://torch.ch/)** is used by Google DeepMind, the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute.  It has been used for some of the most advanced deep learning projects in the world.  However, it requires the [LUA](https://en.wikipedia.org/wiki/Lua_(programming_language))** programming language.  It is very advanced, but it is not mainstream.  I have not worked with Torch (yet!).\n",
    "* **[PaddlePaddle](https://github.com/baidu/Paddle)** - [Baidu](http://www.baidu.com/)'s deep learning API.\n",
    "* **[Deeplearning4J](http://deeplearning4j.org/)** - Java based. Supports all major platforms. GPU support in Java!\n",
    "* **[Computational Network Toolkit (CNTK)](https://github.com/Microsoft/CNTK)** - Microsoft.  Support for Windows/Linux, command line only.  Bindings for predictions for C#/Python. GPU support.\n",
    "* **[H2O](http://www.h2o.ai/)** - Java based.  Supports all major platforms.  Limited support for computer vision. No GPU support.\n",
    "\n",
    "In my opinion, the two primary Python libraries for deep learning are PyTorch and Keras. Generally, PyTorch requires more lines of code to perform the deep learning applications presented in this course.  This trait of PyTorch gives Keras an easier learning curve than PyTorch.  However, if you are creating entirely new neural network structures, in a research setting, PyTorch can make for easier access to some of the low-level internals of deep learning.\n",
    "\n",
    "## Using TensorFlow Directly\n",
    "\n",
    "Most of the time in the course, we will communicate with TensorFlow using Keras [[Cite:franccois2017deep]](https://www.manning.com/books/deep-learning-with-python), which allows you to specify the number of hidden layers and create the neural network.  TensorFlow is a low-level mathematics API, similar to [Numpy](http://www.numpy.org/).  However, unlike Numpy, TensorFlow is built for deep learning. TensorFlow compiles these compute graphs into highly efficient C++/[CUDA](https://en.wikipedia.org/wiki/CUDA) code.\n",
    "\n",
    "### TensorFlow Linear Algebra Examples\n",
    "\n",
    "TensorFlow is a library for linear algebra.  Keras is a higher-level abstraction for neural networks that you build upon TensorFlow.  In this section, I will demonstrate some basic linear algebra that employs TensorFlow directly and does not make use of Keras.  First, we will see how to multiply a row and column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "print(product)\n",
    "print(float(product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example multiplied two TensorFlow constant tensors.  Next, we will see how to subtract a constant from a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-2. -1.], shape=(2,), dtype=float32)\n",
      "[-2. -1.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# Add an op to subtract 'a' from 'x'.  Run it and print the result\n",
    "sub = tf.subtract(x, a)\n",
    "print(sub)\n",
    "print(sub.numpy())\n",
    "# ==> [-2. -1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, variables are only useful if their values can be changed.  The program can accomplish this change in value by calling the assign function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([4., 6.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.assign([4.0, 6.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program can now perform the subtraction with this new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 3.], shape=(2,), dtype=float32)\n",
      "[1. 3.]\n"
     ]
    }
   ],
   "source": [
    "sub = tf.subtract(x, a)\n",
    "print(sub)\n",
    "print(sub.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will see a TensorFlow example that has nothing to do with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Mandelbrot Set Example\n",
    "\n",
    "Next, we examine another example where we use TensorFlow directly.  To demonstrate that TensorFlow is mathematical and does not only provide neural networks, we will also first use it for a non-machine learning rendering task. The code presented here is capable of rendering a [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set). Note, I based this example on a Mandelbrot example that I found [here]( https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/tutorials/mandelbrot/index.md). I've updated the code slightly to comply with current versions of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAIIAlgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDyuiiivROUKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiilAJOAKAEoxnpUixf3j+VSABegp2Jc0RrET14p4RV7U6ighybCiiimSFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAIQCMEVG0X938qlopDTaK5BBwRSVYIDdRUTRkcjkUrGikmMooooKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACjGelPWMnrwKlVQo4osS5JEaxH+LipAABgClopmbbYUUUUxBRRRQAUUUUAFFFFABRRRQAUUoBJwKeEA6800rickhgBPQU4R+pqSiqUUZubG7Fpdo9BTgpNLsPtVqD7EOfmM2j0FIUWnkEdaSk0NSZGYz25phBHWp6CAetS4lKb6kFFPZO4/KmVDVjRNPYKKKKBhRRRQAUUUUAMaMNyODURBU4NWKQgMMGlYpSsV6KcyFfpTaRqncKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKkWLu35UCbSGKpbpUyoF9zTsY6UU7Gbk2FFFFMkKKKKACiiigAooooAKKKKACiigDJwKACnKmeT0pypjk9afVKPczlPsAAHSiiirMwp6r3NIoyafWkI31IlLoRzS+SudhYdzkYH1qQFWGQRilYBgQRkGsq4jFvMiRsyRyHaQOle/h8LRrU7Ws1+JjFcztfU01Kuu5CGHqKQqD7UseAgA6CnEV5+Kw6hNqOwRmRFSKSpaQqDXA6fY2U+5HTWUN9aeQRSVm10ZafVEJBBwaSpiARg1GylfpWbjY2jK42iiikUFFFFABRRRQAEAjBqF028jpU1FIaditRT3TbyOlMpGydwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooxnpQAU5ULfT1p6x45b8qkp2Ic+wiqF6UtFFMzCiiigAooooAKKKKACiiigAooooAKKKeqZ5NCVxNpDVUsalVQopelFaJWMpSbCiiimSFAGTRT1GOe9VGN2JuwoGBSjrRSiumEbsxkxay9Rj3Oj5fjrjoPc9q1Kq3FtHMdzKCcY5r2sFs1e1+vYinNQmmylY3jea/muyqBwsi7SPqK1gciufmh2XKxoM91QnAzWnb3QeJS3ykjoa66mHdRckneS38/M2r01pOGzL1RySLGVBB+ZtowKpy6kkblCCeM5zgVCss11dqUUIYgThj+HI7VzRwMOZxm/lfUzjSna70RqEVGRj6VIm/wAtfMxvwN23pn2oIyMV4demlJoqEiKgjIwaCMHFFchsRMmOR0ptT1Gydx+VQ49jSM+jGUUUVJoFFFFABRRRQAVC6beR0qaggEYNIaditRTnXafam0jbcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiipFi7t+VAm0hqoW+lTKoXpS0UzNybCiiimSFFFFABRRRQAUUUUAFFFFABRRRQAUoBPSnKnc/lUgGBgVSiQ522GqgHuadRRV2M27hRRRQIKKAM9KkC4+tVGLYm7CKvc06iitkraIybuAFOoorrpU3sjKUgqNuhpxaq886xjlgK9jC0XfQhJt6FK+jDgnJB5H15qiJN4LOWCk4YL379T+P6VLc3DSKAFPzjK85yM4/pTJkRLdHAwH6JnJVu+fwx2Fb4qtBSgoPXun0vb+ux6tGLjG0iW3tfPbc6qF/ugYrWtreOFQEXGKoWVwpAXOSOuB0rUQ9K0xUfZw5YbHDiJzbs9h9Np1Ia+brx1IgxjDIplS0xhjntXDOPU3i+g2iiisyxrJu571ERg4NT0jKGFS43LjK25DRSlSp5pKg1CiiigAooooARgGGDUBBBwasUyRdwyOopMqLsQ0UUUjUKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApyoW6dPWnLF3b8qlp2Ic+w1UC9OvrTqKKZmFFFFABRRRQAUUUUAFFFFABRRRQAUU4IT9KeEA+tNRbJckhioT7CpAoXpS0VaVjNybCiiimSFFFKFJppXASnBfWnBQKWtFDuQ5dgAx0ooorRLsQFOFAGKK6KdPUzlICcVVurjyYiwP/ANao9QEzKPLfag+9gc1ktKSxDozliBmRto7f/X7969ulRVOHPZvTp/XQ1o0Oe0mzQjviw2yYDgc81UvJ9zZVuen4f5xTZYJGkQp5YXaFDIcgkdee5oEKRQhroFcdQO3PG49s88+mPam8fTjSSlG8nutreep1xowjLmQySdLSCKd4wZM5wAFzznP6kcA/pUFvqP2iWVHD4dNqoW3dMd8fU5PHXpUd5fWtxbLAolVU5XC9+OvP1+me9VdPeGK6WSWTZt6Hbnsf19OK+VqYtvEQUZLl0v8Arfbbodah7rbWpr/LFcqke7KNg5OR79h05rZjlBUc1lNbpPKJ/PCpIhdfXjtxmohdFvkDkA8bjx/n86+twlWlUotTumtXf9O/4nFWpe1tY3TcRqyqzcscCpqw4sNe7Zc5HTcAMkHqBjgcf55rbByK5MVSi6anBbnFUp+zaQlHWnEU2vFnGzKTuRkY+lJUvWmFcdK55QtqjVS7jaKKKgoCAetRMu0+1S0UmrlRlYgopzLjkdKbWbVjVO4UUUUDCiiigCGRcHI6GmVYIyMGoCNpIpM1i76CUUUUigooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiipFi7t+VAm0hqoW6dPWpVQL06+tOopmbk2FFFFMkKKKKACiiigAooooAKKKKACinBCfpTwgH1pqLZLkkMCE/SnhAPc06irUUjNybCiiimSFFFFABQBnpShSfpUgGOlXGDZLlYaFx1p1FLitox7GbfcSilxS4rRU31J5kJiloorWMLbEN3CiikJrrpU38yGyN+RWNqG0Y5GeuK2JDgVj+ar3RSRRIHwFH93J56jrj0r1vaeyoNtXvodWEi+a/Yragks8Ec6SJHHjLHJHPAPboP8MdhWTc3Ul3L5kpBbGOK0NQlVIGiiRk5VHO3Ibv8Ae9cj9Kya+IzOp+85It9Lq7tdf5fnc9elHS/3BRRRXlmxas717R843Ieo4yPXGf8AJ/Kta1u4tQfaYfLCdGzkDp+XCnrmufrfsC1zpyr5yB1O0/NyF44P4Dpnnn0r1ctqylNQk9Frtfy/4Py6HPWikuZaMltopJ5DIWBJ4yecYxyD61uRLsjCjoBismxDRylA25QAc4659K1lPSvsqsIKilTWn+Wm3+R5GKk3OzH0hFLRXi1KfQxixtFKRSVyyi0ap3EKg0wqRUlFZuCZSk0RUVIVBphUisnBo0UkxKiZccjpUtHWoauXF2IKKVl2mkrM2TuFFFFABTJFyM+lPooGnZlainOu1sU2pNgooooAKKKKACiiigAooooAKKKKACiiigAooooAKcqFunT1pyx92/KpadiHPsNVAvTr606iimZhRRRQAUUUUAFFFFABRRRQAUU4IT7VIFApqLZLmkRhCfanhAPrTqKtJIzcmwooopkhRRRQAUUUoBNACU8L60oXFLWsYW3M3LsFGKKdXRThzMzlKwYorMu7yaCZ42K4IypXgj/P+fShdQyiqBmUgYU8ZJ6V61PBxd1zaobo1GlJdTRZ1QZZgB6k0oINZEty91iMKwII3bcnHIzgipNPmk2ESMCBwO/6961jhYOXJHV2vfoEqEow5m9exqUmRUe4U15AozVRwrvY57NkpamlqzPt7lWcKMDjJ4Gc9M/r9KEvssVcjcO4yAfwNdFKlTcuRSV/8jf6tNK7RdkbjH51kXm3cGI4z0qzLO/lBwh8s5w3bis+abzDXavZuEo8y7HRh6cou4zUwJrWN4wNsSgMfx49+/sOKx624nEoEEo3IVKqBxjnPOOozWde2Mlo+Sr+WSQrMMZr4DNqMvae0WttH5W0Xy/XTsepTfL7rKtFFFeOahW1py+TaeYMRT8Yd12/KeeP72eOT/Sqdlpst1tkcFLfJy5B5x1xV2W4MgVMYVfU5JPc5969zJ8JzVVKps/y7/5f5GFV8ytHpuaFhlY1BJIHOM9K01YYrCguQgGeKvxXJ2BypCH+Lt1xz6V9zWhBpRTSvt/wDya1KTbZohqXINUjdKq55/Cq8OoY+d9xjZyN3ZeK8+rhYx+J2voYxozkm0jWoqOOVZF3A5H8qkrzqmHcWRe2jDFJilorH2Db2HzCYFGKgu9hhO4DA55qtpt20oWEhRsT8/pXRLL17PmW+v4GkVJxcl0LjL3FNqUio2GDXi1IWZrGVxpGRioSMHFT0x17isZI2g7aEdFFFQahRRRQAyRcrn0qGrNQOu1sUmaQfQbRRRSLCiiigAooooAKKKKACiiigAoop6R55PSgG7DVUseKmVAv19acAAMCinYycrhRRRTJCiiigAooooAKKKKACilCk9KkCAe5ppNkuSQwIT9KeqhfrTqKtJIzcmwooopkhRRRQAUUUUAFFABPSnhQOtUotibSGhSfpUgGOlFFaxikZt3CiilAq0ruxLdgFLRRXbRjroZSZRv3jWE+Z0PFZ8Af92ZFDQqcoS33T259On/6+K0LuEyZIbjaVx25rFuF2TfOF652gYH0r169OpOj7qVl167f16ndhOVx5b6ly7ibCiBAgTDOfulTz6/09B7VXgacHZGvzDHB46/5/WjVZ7iExSRMnlr8wfIy/br1PA5x/hVR9YbM2xCd6hQzEZ47njv6Z7968mGZrCylBys3rtf7v+DZHTGDlBaX/rqa8d8CSPmAJO3PcUy6ufkIBGfT1qvcSpPEs4naV+AclcAfQdOo/XNRsI47YT3DPh8hdvYjH59a9qGYUvZOUt/Lt3+7UyWHSalYsCWO2sWlUumSeMhS4x2P1x39eO1IYze2wmtgzc4wHBA49MDB6flVfUGsUhEAkLHaWDBRu56Dp/PHH1rLhu5beJkhOwsQS6k547f59a+YrZhKhWtfTXb4k9d/k/u7HRGnzLmjv59jZmlNkyAIW8xlDMSAmecgkYGPY9s9OKik1C1gHlxkyxMCGTPIYdD0wfzrFZmYKGYkKMDJ6CkrhebVVfkW9t9e1/J3639DRUF1/r/hjV/tkpwluhw3BbHTHAxjjnk4/SopNZuZj++WKQdlZeOoPTv071n0VyPG19bStf0/yL9lDexI8qsDiGNSe67uP1/zmnxXCxOri3hYj++CQfwJx/8Ar+lQUVj7WXNzdfRf5F8qtY0YtYmiQxiKPyzk7Rkc4x1z+NPXVY2dvMt12s3YAkL6A/549Ky6K3WOxHMpOV7dyPZR1stzdivbW7LQysIlAHlseo/Hjp0p9vdi6EyBd3lrwGb5SB9PoP16Vz9PjmkhOY5GQ+qnFdcM3rXSnt5fh5fq/VEOguhuLHIVZpJRF5bKNzHAyc4Ofw/XNStKl4QVkPlwqRvCk5PGenQYPbOPxrHuNRlurfy5SxbduyG4P4fnVvSoohazymZfNIwkfmBfx654/wA+3ovNJYqulZNW+7ys9/8Ah7Gbp8q5nubGnybE8vcrKuMMo4PFaG+sGEywA7gQRj5MHPP9fb2+mbX20FRg9ewr6iEaVaN4yvbc86tQbnddTQNzEGK+YMjqM9KUTKwyrAj1zWRCjzyNuhJiP7w/eyfTGO9JcvLDcyPEriMsVBPc55/z7Vze0owquE1a3W+ncf1RPSL1LGoS5XYSQh6kf4VJpZKq4cAMG5A7e3/66oSrc+YAUyV+bIGRx1qWQG3mHlO4D4Kr1b3yT0OfrVTq0ZVuWDvdW073/wCH62NXS/dchucGmsMrVaxhlQvJM+5nxx6AVaPWvEx1GNOfLE5I6OydyKigjBoryjoIWGDikqSQcZqOs2rM3i7oKKKKQwpki5GfSn0HkYpDTsytRSkYJFJSNgooooAKKKKACiiigAopQCxwKmRAo96BOVhEjxyetPooqjJu4UUUUCCiiigAooooAKKKVVLUA3YTrT1j9fyp4UL0patR7mTn2DpRRRVEBRShSe1Ls96ai2K6G0U/YPWjaKrkYuZDKKk2j0pafsxc5GFJpwQd6dRVKCQnJhRRRVkhRRSgVSi3sJuwAUtFFdEIW0Rm3cKQnignFRvIF6nFejh6Db0I3GydD9KymEb3MqyOqdMb+nX/AD+tX3mUjrWTeSKzdea9aWH5qDg3Y68MmpFPV5ZXKLskS3zlMk7TwOg/z1rLroIIft0DQNCpP/PUgkjjjnt/ng1kXcMEEjpHKXYMcAAEAfXufw/wr4LMsNNTdXpt87dPL0v5nrUpJe51LWnXKWkLm4VjE4OxfU8cj8utQXWp3F0ZFZgI3wCu0dB056/rVOiuR4up7NUo6L8X6vt5F+zjzczCiiiuUsKKKKACiiigAooooAKKKKACiiigApySNG25Dg8fQ855HfpTaKabWqA0bfUMyyPdPISwGNmByB1z27dOueatrCzTgRkhPvBgwJAHOcg4rDq5YXfkOYpHIgk4fjOPfH+FexgMznTfs5ysn19Xrfv+FvyxnCyvEde3zyvJGuBGTkjqC2ME9SOveore/uLZiUkboR1qB0KOVP4HBGR2PNNrzpYmtz8zk7r+v18zRRjaxvWstykDPcMu5wHjIVSeBwT+B+v9ZYbe4MeUw3O7a3rjr9abHCXs0hGI/LA3AkHc2OvHAGO/51qWKEI3JI3EDOMjt2NfZ5byKi1Uu5Lv8nv6+X3nnV6jhG6LUKlYxuJz7049adTT1rhxc+aVzz4b3GOOabT3HFMrzJqzOmOwhGRioanqJxhjWcjWD6DaKKKg0CiiigCKUcg1HU7jKH25qCpZrF6BRRRQUFFFFABSgFjgUAFjgVOqhRgUEylYFUKMClooqjIKKKKACiiigAooooAKKACelSqgXnvTSuJySGqnc/lUlFFWlYxbbCiilAz9KpK4gAJp4UClps0nkxGQqWA5OOuK6qNB1JcsdzKUx1FKrBlBB60uK2nhpw0ZnzobRS4oxWfs2PmQlFLgUuKPZMOZDaKdRT9l5i5hMUYpaKtQihczCiiitFFvRE3CkJwKWmsa66NLWxLZQudQEUpjCEkDJ47e1VWeS4fLsioRtIzkjBz0yPQfnT9QBEyPgYXue3vURd0tpRBxIoIxgqE59eO2ck+gzXZiZypRcdk9tN/Lfy8mehRhFRUorUrK0/lk7DtxngY49cenHWnRyvHbNOFBRMiQ4Unn8Mn/ADz6Uzq+68SUxYjA2lcgnoRwce9SapdRRSSW9uBjGCwP3c9R068euOSK8evnEatG3NpH5O9tNHv8/u2Ozk1ty7/0yO/vF+WOFPLOza464zyR9eSD+VZlFFfNV8ROtK8umi8v6/M6YxUVYKKKKwKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAki8sttlBCn+IdR/wDWrWazsbY7kWWYn7pc4UdCOnX+XNYtaFjeEbIJBGQAQjOThOc9ADn/AOvXfgalKM1GpG+uj6+nzM6ie6Na3UXDq7quQMepP1rYiAUAAcCsWGK4gfDYCjPzduM/l07+ta1vJvjBIIPcHqDX3kpUZ0rUduv/AATxsUpXvfQs0hoBzS14dem9TnixpGRUVS1G33jXm1F1OmDEqOQdDUlNf7tYvY1juRUUUVmbBRRRQAVXYYYirFRSj5gfWky4PUjooopGgUUUUATou0e9OooqjDcKKKKACiiigAooooAKVVLGlVdx9qlAA6U1G5EpWEVQopaKK0MgooooAUDJqQDAxSKMD3pa2hGxnJ3FHWkkRXQqwBB6g0opk0nlxFtpbHYda9LBQbmrbmE9XoZFxvtZ9kUrBWGGBOdoz1/WteI/IOe1Yc7tc3APlttByVwckc8/59a1YJVMYwRjFe5OHtIyiujX5G9eL5It7luiow/vQJVYkBgcda86WEfY5bskoJxTd1Vbq4MKbgM8gHHWqpYTmY0nJ2RPHcxySOinlDg1LWBG2dQZg4PPJUnH59/yraRwR1rWWGUoc8VoaVqXs2rMlopu6jd7VzPD+RjcdRTd3tQSTVqkwuKTUbHjFKTiqd5M0aDbjORnJxXZQpW1KhFydkQ3kyAFT1I6VnWwDT7NjvuGMI+09c/0qdZ4/JZpuc8SHbnA5Awf89RUKzWQtrkJJJvQZJ2hh6DB/HGfoa58fjo8nsuW3rZad7drf8E9WjT5E0UdStYLSYpGzEkfd9Pfp0/H8aoU6SRpXLucsabXwuInCdRumrR6HfBNRSe4UUUViUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGvZxiSykuCS7s2zDZATvwc4PH+fXSs7uOOMKWAx61haZIEvFVgNsgMZOCSM8cYq019BBMY4N3p5pP0zkDII69Pavq8szWnRoWqbvS777vu9b3OOtR524s6IXSFMhhj1qK1vWe48twdrn92duM9zWYoa6YmGRWR2yyrzsz7e2auWkqpeOGhZXLBBhcYGOP0Ga9l1qVZKMLaq79ey/z8jilh1CL6s1jUb9RUhpj9K+drqzZjTYykb7p+lLRXKbEFFFFZHQFFFFABTJBlPpT6QjIIoGtGV6KKKk2CiiigCzRRRVGAUUUUAFFFFABTlTPJ6Uqp3P5VJVKPczlPogoooqzMKKKKACnKvc0KueTT60hHqyJS6IKKKUCt4x5mZt2FpGGRS0HpXoYfR3RjIyNRtmdwyqCo64OCaqCSSI7Q4UHOCewz36fXpW44zWXeRqUPY4zXvU4OonKMmm15fLc66Fa6UJK6IjdSxxAt3xggHGPXJoguWglJkG3cAaZaTMJz8v7tVJKqTwPbn/635U5prK6Mm+URuHOScDd6Dnnv/j7efLHypTUavS66Lp3vb/gXOp046q2jNBLtX46Edciql7JvTg4IOQe9VJgYZWKspVicFOR9KaY5mQMcYYZGXGT+H4Gu2OMw/IufdrVfmRHDcsuaJND++aZo4SHA3ZB4B/oO/4VZhuZIwhk6MOo5HUjH6VXJSxgW5KEE9cnJA9McZzj+fpUdrOLyw8tYi8y+jH1649ADge/bmvKpZlGnUUG9N7N9P62XkzSdNTW2hovffMETbuIJyTgcDNSWt0ZVO48g4yMc/lmsqBZY5juVdnAfdggjP8ALpzU7/6K4kjH7pum05A/HueDXoQxVOrWcPs9H5mM8NFRstzYD/jSlqzI735lVgVLcjIxx61aE2R96uhUYy1jqjjlRlF6kzNgZNULx12EN+NTPJ6HJrPllQTEuARjK5Gec/r3rZ/uYOdr2NaFO8gjuJJIhFMP3G0gNsBIGMcf5796w7hY45CkTBk4O7rk47HFbN9eXNsIymGhAyfmI3enofc47n8sCvgc3rKc7W1326eXlv8AcetRWl7WCiiivGNwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAFR2jdXQkMpyCOxq02nXBG+KNpIzjDAY6gHkduo9qqVr2RintEh81Vkj5wVIABPr+I9O1duBowr1PZVHZGdSTirotQwiMxLaRjACl2IAZjjk85I/UVq2ds8bebKw3kYwoAGO3QVHYqgiBAAyAcVoDpX2dXDwwkOWG9rHjV67k+UDTW+7TjTW+7Xg1dWyIdCOiiiuU3IW+8frSUrcMaSsmbrYKKKKBhRRRQBXYYYikp8ow/1plSbLYKKKKBlmiiiqMAoopyoT9KLXE3YaAT0qVUxyetKAAOKWrUbGcp32CiiiqICigAnpTwnrTUWxNpDAM9KeFx1pwGKK1jBLchyuFFFKBWsYuRDdgApaKK6oUn0MnIKRj2oJxUFxKY4ywGSK9HD0G2kidW7DnYCsm8YFtpYqMYz2H4fn/nq978biCMEe+f1FU5pGkAcqRGTjd1/ya9Vezp0ruVk+q/Q7aFGSlqiSDa0BSJtrN8rgdW6/wCHT9ea593aRizsWY9zXR2b/Y1eWSb90pwQM4J9/wDP0rEvZ4ZpGMUQDFiTJuPzfQf48mvic2fNFe9126vz/wCH+R6VJ++0kakN1BHpkTXRy4HGeXPJAwPQDB546+tU5dXlMiNFwFA4bHPHTj/PT0rNJJOScmiuGeY1WkoaWtr108/l6FKjFb6lu41G4ulZZSpViTgDABznj/Pc+tNgvpLZQIkRT3YZBPTqc+361Worn+tVufn5nfv1NOSNrW0NBtYuHR1YkbiSNhxgnP8Aj/nrVuymt57NLeQ4lyXZ+rHG7+QrEorenj6sWud8ySt+N+hDpRt7uh09wsc8Jmgzuh+V+O+fy/Lp/KPzXhfy5QAwHI64rKstTltJGY7nD8t82CT698/iDWgXS5tluMkSYwcY5PH+P6dMdPp8rzaPKo39V28/67nNKi07PboSvP8AL1H0FR24Etxg7zJ1QLxz9e1Vt1T2zKscrklCBjzM8LkH3H1HuBXs43Ex9jaL3/r0+/QlU+VMparPcySpHc43ICeGBBz0PHTjHXn86z6dI7SyvIxyzMWJxjJNNr83r1XVqObO6EeWKQUUUViUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVp2Enl2MgaYRh5Aq5frnGeAMgdMnp0rMq/aWcdzaM8jtFtkC7yCVOR0xjrwO/euvBSqRq81NXl0/AzqWtqatvcP8ALHDltw3IWAXI/GtawkeW0R5Pvd6zIYgY1hgikZBx5hflh6HgHHt7Vr20XkQLHuLEdzX2dWrVnR/fKzsunXqeRieTpuSHrTH6U49aa/avCqvczh0GUUUVzGxE/wB802nSfeptZPc3jsFFFFAwooooAilHQ1HU0v3fxqGpZrHYKKKKCizShS3SnKmeT0qTpWiicsp22GqgHuadRRV2M27hRTgpNKFA96pQbJckhoBNOCgdadRWigkQ5NhRRRirSvsSFGKXFLWsafchy7BiiikJwK66VK7M2ytPerBKFkGFI4b39KlSUOoYHgjNUdSiLqG6qvVR3qvBcmEbHbI6qc9RXtww8HZW3Wn6myoqVNSjuaxb0qvMwKkdsVA90SwVMMzcAZxmqpvA65rppUVGVuoQoy3IJGMcmw7ShbJ3KD1684zUl7efYogYIcxPwD26e6+7Y57dKiRY5nUZYyEj5COG56ZzxxTNTvkFqttHHImQMbgcbcdQfrkenB9a+ezipCLlKPu9Nuu+i8+9l+aPSjG/KmrlPUb4XMzCL7ndsYLdPyHA4qjRRXx9atOrNzm9TsjFRVkFFFFZlBRRRQAUUUUAFWLW8ltC/l4+YY56j6HqO1V6KuFSVOXNB2Ymk1Zm9cQlo1uYxmN8e2Dj35x6e2KinM0GmvtdgHzuQA4APGc9PajRLpObWcsVJygJ4Hrxn+nrUniErGIYV2Hvx1GM/h3/AEr3HiJzw7qraz+Telvx/E5uf3lSktf0RhUUUV4B1BRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWhYNEsLr5hEjHO0A84HA/U1n10Nmscc8aQCJsKMSlRlvcYx64/DmvVyijOpiE4NXXcxrS5Ymlp0m8OjYDocEA5wPr+daFV7S2W3QgdWO4/WrB6V9Hjark/eep4c3GU247DaY/Wn0xvvV4tTY1juNooorE0I5PvfhTKfJ978KZWb3N47BRRRSGFFFFADZPuGoKsN90/Sq9JmkNgooopFmhRTguaeFx0FdUYNnnOSQwJ608ADpS4oxXRGi+iMnMSlwB1NLVW9kC27HqVwwH0Oa7sNg/aSSZHM27ItYFFQQ3CSAYYHjPWpsitKmClB2sS5Nbi0UhYCoUuUeR4wfmQ4NOGDnJaCu3sT0U3ePakL+9aRwr7CuxxOKaWqpeXJhhLIRkEVCt6GTJOCOuDXoUcJ95apSa5ia4cYOay4irPKhba3Gz656j3/wA4NE8zSy7UblsAc96fBb7C006/LGch9wAbn37eh4qcdVhGm6H2tP6v/X+foUafs43YyO1KMsjyfIpO4rlSMemR60n2WMl9kp+7lQ/y85/L1H1rLurx/tTtC7p2zvyc5zkHJ756H+dVxPKrh/MYsG38nPzetfN1s55ajjdytpe/Ty076/gdcacnqb0FvNAJZWADRjKnrz047HrWFdSCW5kcEEFuCARn35JNalrqBntJIGhDBIizgkkPgcH27Z57celY1cWZVo1IQlGV+a77eVn5r/htyqXNzPmQUUUV5BuFFFFABRRRQAUUUUAFFFFAD4pWhlWRPvKc89D7H2q1qYfzo2d93mRh1Psc4/lVKp7klkt2KquYhjHfBIz+ldNOtJUJ0r6Oz+5kNe8mQUUUVzFhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFSwQGeTaGCjux6CqjFzajFXYE+nweZPvaJnReh427uo3Z4xXSabbbYkcqVIBAXkD8v8APaqthYqQn7vbGOQhOcnHJP8AkdK20GBX2uDwCwNG8177/A8fF4nm92I6kNLSHrXHXle5xwQlRt941JUZ6muGpsbw3EooorI0I5PvfhTKfJ978KZWb3N47BRRRSGFFFFACN90/Sq9WarUmaQCiiikWXt952t7f/v+f/iKTzL7P/HvBj/rsf8A4mpQ/rTwa9WlWil/X+R5Uovt/X3lcyX3/PvB/wB/j/8AE0wy3+8DyIcY/wCep/8AiauZpc12U8TBf0v8jPbov6+ZRMl//wA8If8Av6f/AImsm9kui8nmRxhtnA8w+h9ua6Sqd9ArwOQo3HgE+vau2lONaLpqTTfp/kvzNKNVQmrxRhadLel3bYGck7i52n+Vagm1HnMEXXj94f8ACrkFokRLAfMep9asbRV0+WhTVJzcrddP8mOtiIzldRRjCbUirboI+p6tj+lVo3uBNujRTKc7xvPHp/D/AFNdCUBqvHZrHNJJ3c5+lbRqwdvfat6a/wDkoRrxSfuoyXn1Dcm6NQc/KA55/Slkn1HySWiUe4Y8fpW55Y9BSFB3FONWLfxv8P8A5EX1iOnuI5t5Lp2UTKAhxn5j0/KmygifaUjVMnO2Qnue+P6Vs31sXhIRcnIqBbBRH8yjP8qqeGdbarJad1923zOqniYKN7fIz7Ybsl1jLD7gaQjJ/wC+eaivZb4zyjy/lCZcKNwXIyTnHBzk/WrE0QgmBAB2kEA9/wDOKnilW6DRTBAGOIwQS3XJ5HP48ZrysZhKkG488rXXvfJ6PTz6HRz399L+vI5qip7qDyp3VCHUDcSvRfbv34qFVLMFUEsTgAd6+NqUp05unJarQ7E01cdF98/7rfyNMrVtNOkEDzMIyHiYDJGFJ6ZJ4B/HjPPpWVW1fD1KNOHOrXu/yJjNSbt0CiiiuUsKKKKACiiigAooooAKKKKACpJP9XDx/B/7Mfb/ABqOp7hSiW6sqg+Vnjvkkgn8DWkPgl8vzE90QUUUVmMKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigCaBbds+dI6nnAC8dPX/AOtWtZog2gABTyMdG68+/esOp7W48h/mDNGfvKDj6H9a9bKcwp4OtzVIJrv1RlVpucbJnZw42jFWB0rIsb0HajOr5HDKcitZTkV9hiWpxVSDumeBVg4Ssx1NPWnUh614NXYcRKjPU1JUZ6muSpsbwEooorIsjk+9+FMp8n3vwplZvc3jsFFFFIYUUUUAFVqs1WpM0gFFFFIs0KAcVGr9j+dSVsmcLVtx4f1p1RUAkVqpvqQ49iXNDBXGGGRnPNMD+tODA1vTruLvFmbgPoptFaOs3uRyDqKbS5pqsJwFopM0tbxqslxEIBphX86kpCOK7aNdkNWKNxGCDxWXEiCSWRxkoAyj8f8AP+BrR1GXy1Cjgt/F2FU4ITN8/Kqp+Xn9fr/n0r1KsXXpKknvr6Lz9Tvw75Ycz2GJcxu43goWJJkY8ryT2APX+ZppktgzkAyEZZSwwCxPJ4798+nFW2tPLdXjVdynIB6ZqqbMquD1rneWKTspad9G77fkbxrQeo+O4a5hmhMSkMBsVR3Ht34H/wCvpXPzx+VO6DOAeMkE47dOK2otkMqPvYSgjAIwoOe5z0xVfVLErELoSrJwM4AHHrnjPP44IzXzebYOSi+XXlb9bdXr09DppyjGWml/zMmiiivmzpCiiigAooooAKKKKACiiigByI0kiogyzEAD1NWdQK+bEqptCRhemM4J5PvVvRLVZZmmkcBU4A9fXv8Ah+NTeIYlLRTRrgYwTkd84469jXpQoRWDnKV1J2a80n/nr8jB1V7VR/q5h0UUV5puFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAXtNlYyCAsmwklVZc7mxgAemePyrpdPut8aJIwMmM9RkiuMrpLRSjRvbq5g2AqCcYyec5Pcj6V9NkeJnO+Hlbl8zhxlKMo3Zv0hqK3nSdMoQcHB9jUp6V1YqnytnkxunZiVG33jUlRt96vNqbG8NxKKKKxNCOT734Uynyfe/CmVm9zeOwUUUUhhRRRQAjcKT7VXqw33T9Kr0maQCiiikWWacrlfpTaKowauTBg3SlqCnq/Y/nVqXczcOxJRQDkZFFUZihiKeGBqOiqU2hOKZLRUYYinBgfatFNMhxaHUuaSitE2tiGh1FNpQa6KdXUiUSnc2RuZRvbEajgDqTU0cIjQKM8DGanor1I4xtWJcpWUeiIitV5kAU+lXCKr3EbPEwXqa7cNXvKzYoPUw5I2lkyiMUzywHGPrUl/bXM0KxWz5iTqBn0wecf73GfwqWSwYsxY9TnA6frVZxLEgj3bUzzt6nr/ifasMfgquKTnptsnqvn5nqwqRdlF7GZf2LWcpA3GPOASMEH0P+cGqldHAkd/5kLq4L/wAWcnHHGcew/wDr9Kxbu1Fux2yIy5xtDZYemfX6j9K+Mx2C9k3OHw9fJ9v8jrp1NeWW5WooorzjYKKKKACiiigAqWG2luN5jTIQZY9APxp9rZS3b7UwOOrZ/QDk/hWrHGtnaqAxWYgnKjnt3+oP+Hp6GDwE68k5JqL/AB9P89jOdRLRbjpZ/Jto7SPG1Op9/Tjg9eveq93IZrDGCWXrnGAPbPOfp/jSEFjkkk+9WLck208bSERkEmNScnAOe47fnX0dfLfZ07vqmvTT8l8zLmio2Rg0UrKUdlYEEHBBGCKSvjHodIUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVoaekaqZw8izIcDaRxkcH19RWfWjZXMEFo4mZvmkB2xgbsYOTnHTkd/WuzAypxrqVT4Vv+BFS/LodDpqFfMduC7Z2jp/nrWjWLDcNHGssU8bwMcKWzn6Hpjoa1oJlnhWRDwa+vxPsqkeajsv1+88OtCSlzPqOpj9aeetNftXh1Foxw3GUUUVgakcn3vwplOk+9Taze5vHYKKKKQwooooAbJ9w1BU0v3PxqGkzWGwUUUUiizRRRVGAUUUUAKCR0qRWDfWoqKadiXFMnopivng9afVp3MmrBRRRTEKCRTwwNR0VUZNCcUyWimq3Y06tk09jNqwoNLTacOa6ITvozOSCmsKdSHpXdQm7mTRC6Z/pWTeKA+WJ29cDqf6d/899hzWXeMoQ554r3aF5wavY3w7akQ27+Rb70dyM5bDcL1HI/LjvxzXOMWZyXJLE5JPXNb9oj+e6BgqspVmDAjn9D9PrSvYWkZke6LSMXIAOcr6A4I44wPb07fJZlhXWkoweqb11s9tfX0X3HrRmoSd+pBBYQ3emxyOQkp/5aDrkE8Ed+Me/IqjNps8cioqlt3sRjj37dfyrRuGMkuxEZQnGMk/z5xQLqVVUbRuUYDZOf5/5wK2nlHtYxlybrdbu1te2vf9QjKS1v8jIltLiHd5kTLt4Ofw/xH50JaTPGHVVKk4B3jrx7+9bsy/b7VYBIpbPJGc5z1A49efqfSoNPhaztDcq8YlbPBPPBHGP1ry1lsXV5Vdxt3Sd+1vy/4BXtXbXczW0+4RHaRNmzOc/j6fSrtjY2/wBjS7mIKsSpHBwfmHT6YP1xVuCe4mmEYBKk8gE/KP8APfr+ZpZkWVxDEF2jkleQT2Oevf8ASvRw+UxVZU4r3ktb69d+y06eZE6j2k/uJZCtlDIInYy3B3k9Oc8+49OT9O9QMj3EpldQGPXb0NTraOzo0jZKjGenHarYhAHINfS4LCRw8feXvHHKqo7aszXgwOmKbakRTli+1h0BGQfr+hrReIY+X8qovDun4YqQAVwcEnPb/PaujGcsqEm+hVKpzaMytTSX7QJZYViZ8/KAR07nPfp0/nVKtzU0uLjykCAoRwztyD16nA5GOvv6YGHX5vmFH2VZ2Ts+/fr2v9yPQoyvBBRRRXCahRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFaFnFFLZuZY92yQEFSQeeoPGMHH1rPrYsAsFmJGMxEmRs3YU88/8AoIz0/Wu/LaXtcQoNXT3M6rtEtW8ciENFl1CbMOMbhjHODWvp4cWaeZnd71DYhfKUcE4FXx0r7XF0qdGHJBdDxa9ZyfK0B60x+lPNNb7tfOVeoodCOiiiuY2In++abTn+8abWT3N1sFFFFAwooooAjl6AVFT5fv8A4UypZtHYKKKKBlmiiiqMAooooAKKKKACnq/Y0yihOwmrk9FRK+OD0qWtE7mLVgooopiCnq2eDTKKcXZiauS0CkByKWuhPqjJroOpDwKBTJldoiEbax6HGcV6WEtKaTMZLoZ9/dNEwUYAbjce1UG3SnO1mUZHB+9z+GOPb8KdOJI7kLKwYtwGIB4+hH0rUt4FWMYHavcSvzczaitNHb1f+XzO3mjRgmldmYEuDCAm5AowACOnXqO/+e1Njge5mPmAAgDJHU55rd8tfT9KRLdIyxVcFuTWUXQptNL/AIPb7v1M3i3Z6FGOxCZJGc+wFV7yIRxk4rZ2+1VLy3aRBswGDA5xyK6KWJT92PyM6dZuacmZcAa2eb54y4Xbt757j9D09qmjhedI1YbVUcqB3BOP0NRxqq3xULtG7oWz+XrW0kYArmpQpxSrSu5X9NfQ6cRWcNupmvYlTuRQRggoeA3p+vNS2dqY1JZcZPAx0H5mtDb7Uu36U3iYqTkt2cjryceVkYTHQYoKmpNppMYqFXu9zG7IWUN9aoXkSlSTWoRn61RvYmdBtAOTyD6V2UaqsbUZe8ilD9qMfztKsAUncBz65/r71g3Cx+YWgz5XHrwcdMmt4Qma2KSSSBQDvHUIAD05/r69aqrYWZtrl1kYkjhQmdvfgn6fXANfJZphnUlaCS69VZ9v8+n6+xTna7f4GLRSsrIxVgQR2IpK+aaadmdQUUUUgCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAAJOAMk1dOosiCKKOPy16F0GTwB+GcZ4/M03TYjJeo2SFjPmMwPTHNTyachlLQyBoieF6N2JAHOO/J9K7sNRxDjzUd2/638zOTjezNOJ2j8trb95G4BLYPykjoccA8+5rUtboy/JIjJJjO0qenrWJEsdlJtjhOVbEjH5s46444q/aQym7cmTaVYZAUDIxwOPY19dR9tOKhV7X/AOD/AF8u552IpwcXL8TVNNb7tONMfpXjVtGzkh0GUUUVyG5C33j9aSiisjoCiiigAoooJwCaAIHOXNNooqTcKKKKALCsGGaWq6sVORU4IYZFMylGwtFFFMkKKKKACiiigApyttPtTaKAauTAgjIpahBI6VKrBqtO5jKNhaKKKokUHBqTrUVOVsfSrhK2jJkrkg60McLzSUy4jM8LRggbuCSM4Fejg5RU0pOyMJK5j3jrPcfugXUYL45GP85/M1sQqPLGOmKckSIoAUdKf0r06+Lp2ahu+vpoFSfOlFLRBRRmjNee6zb3I5QpCM0tFONaSYuUrRWaRTSSjkv69qsgYoorWWJlLcbvJ3bCiiisvasVgoooq41U9xNDSKY65FS0xhXdQqNMRlXluDuOcE9aoW+FmyXjVQMnzOhrQv2bzFjAyG6jOCfYe9V2UxW8zxL855IOADzxg+hGTxg8cdq3x9RSp8ijd9Xbbe39I9ShJqGr3MvVHtZbgtA3IAz6emOB1/MY71n1f/smf7UsJKkEZLqCQByfTngfjUuqWCRSyS2+3yhyQDn0yemO44z39K+FxFGvUcqs42fX/P8AzZ3QnCNopmXRRRXCbBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRV2ys2mcSMgZAM7N20t244Pf88GtKVN1JcqE2krss2BiWzkVQvnqxLFsDgjGAep+nT+ujZ2gdd4ZhuGODioo38+QgwIFOeAo98c/l0/Kta1i8uIA4z3wMV99gMO8NQcakdNN1/XzPLxVblWm5H9iTy9uMj3qOztJFut5yqR8LwBuHTB79u9aQFLU4jFpqzWq2OKNWdmu4hqN+op9Rscsa+eqyuaQQlIfun6UtNf7tc72NluRUUUVmbhRRRQAUyQ4T60+opTyBSKirsjooopGoUUUUAFKrFTkUlFAFgEMMilqurFTkVOrBhkUzKUbC0UUUyQooooAKKKKACjpRRQBIr54NPqCnK5HXkVSl3M5Q7EtFICCMilqzMUNj6U8EHpUdFXGbRLimS0VGGNO3+1ae0TI5WOopNwo3D1p3QWYtFGc0UxBRmiindoBc0ZpKKpVJInlQ6im0oNaRmmS4tC0hGRS0V00qnK9SGjMu7GSWbzFftgA/w1XQvbvsZCoAyWQ9ATjgZA9PxraIzUEsCSDDLn2r0/3dZNS0b66/1Y3p4lpKMtjBDTkHcPmA25ZeQP8AJ+tOSKWS0MSRk+ZkbwhJUegP58e/X01GtkVcKAB7Vl3MflsSCQTwcGtquX050kqWjX5Ws/T+tDsp4jndihqFg0JV02tlNzBBwOxI9s//AKqz66G1PkW7zyrG8OCMNtz3456f54NY939laR2t9y/McLj5SPUHr+BH+FfE5hhadN80Gk9mv1Xl621eh205u/K9fMrUUUV5ZsFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRVmytftU2GD+Uo3SMozgVdOnKrNQgrtik1FXZFEiM2ZHCoOvqfYVsu9pMix210oCdI3G0dhwT6//XNYjsGckDA6AcdPwpoODkda6cNi3h37qT136/8ADEyhzWdzp7T90yq2MkEjB7f0/GtiPkcVgq6i3WaZFcyKpBAI28Y2nOT+PfrWrYybkIBJAPBxjP056V9zTxUsVSbtt17/AOX39zx8VTsuYu0hpaQ9a8XENps5oCHgVFT3PFMrgqPWx0wWgUyQ9BT6ic5asZbGkFqNoooqDYKKKKACq7HLE1M5wpqCkzSC6hRRRSLCiiigAooooAKUEg5BpKKAJ1cN9adVapEk7N+dO5nKPYlooopkBRRRQAUUUUAFFFFACgkHIp6uD14qOimnYTimT0VErFfpUgYGrTuZOLQtFFFMkKKKKAClyR3pKKAHBzTgwNR0VSm0S4oloqMMRTwwNaqaZLi0LRRRVEig0tNpQa1hPoyJLqLSEUtFd1Gepk0QSDg/Ssplje5l85WdRjCgnucfzrQuphHlcEnaW/AVi3LM8xVgVOcHPr0+lexNr6vJc1n/AF+h2YSDbKmr+akixsyMgOFKjk4A6nHNZtburiZoYYo4Sbc42Mc8H169we/+NUG0uYeaUYMqAN0IJB6fjz618RjcLWq1nKnFtWXy02T6peX/AA/qUqi5FcdYWYv0dGZUMakhsfofXrVeeyuLcvvifauMvjjnpz0rWeOK3thDEXV85ZWBB6fXrwD+IxRIy3Nr5c0zKwByxyS4yCB+neu55S5UE5KzXVdX2flfS5KqO91t2/UwaK2b6xshEJ4mdE2kA7epHfHvwO3r61lLDI0RkVcqDg4IyPw64968WvhZ0pcu/p/lujaNRSVyOiiiuYsKKKKACiiigAooooAKKKACegotcApVVnYKoJYnAAHJNTSWkkMPmSYT5tu0g5z/ACq9pn2VrWcOi+eq5BYkBh1xweuQOP8AI6qeFlKoqc/dur6/1/kRKdldala2sfMeRZ2aIooJyhI5HGfTtx9a0ElSFyiKvkHhgAcMPXBPapUeS4JYg+YSPmAGPr/THQ8enM4s9oBXIZTkGvr8BkqpwfPo/wA+qv6eVrnJOuk/eMG7tHhmfC5Xg/L2yM46k8VDFBJMcIhPGeldDEzFnim8pojwS+3hv72D1NNnMsdy8duwCqxYFBg8nkH8RXkyyNe39nF3t0/Hfp001fc1Vd7MgtFmmtCr27RiMDYFQ4ckdcevGc9Ofzs297KqBY03Nj8OBUcks4lysaqWPChQcn1+vpUkrI8gEPzKpB3KMo5Izk46H8/wr28LCthP3Etb/PTa9l923bcwmozWqNuJ96An+WKU9aq2Nz54ZGQoydRVo9a4swhyVLHmqLjJpjHPOKbSk5JpK8eTuzoWiA8DNQdakkOBj1qOs5M2gtLhRRRUlhRRRQBFKeg/Go6VjliaSpNkrIKKKKBhRRRQAUUUUAFFFFABRRRQA5XK+49KmVgw4qvQCQcii5Ljcs0VGsgPB4NSVRm1YKKKKBBRRRQAUUUUAFFFFADw5HXmngg9Kho6U1IlwTJ6KjEnrTwQehq00zJxaFooopiCiiigAooooAcG9afUVKCRWkZ23JcexJRSAg0tamY4UU0U6uqjPVGUkUb+JZIjuYrjuDis2ACQhGiiEfTc3G4+gP8An+lW72GeaVztZY1GPvZ3e+B/n+VItjJ5cb5y6YKq3AHTt+Fe1Up+2p8tlp176bL/AIOx2UZqnBXlv+BHeStERIu1y/LNkZU89MdOnqelVopbjc0gG8sADu6Ef5/yasPC9riRtoXduIABC8jpmpNORmTJj2jqMdMe1Rh8NTjNwqrXRr+u5q6ijT5o6rYZ9leRsyyF9uQu70zUd1bYTIHP14rYCYFNkiDqR616MK0IrkjojjWIfNdmLJawy6eRlSQSFfbgDjOSR+A69+nanRMmmW/lw+WZSwLHBJP4kD3GP/r1OlrPErCPIJ5yDjnPqOcY7fjSLaMxJk3AZ4Vmzj+n+RXlQyzmq80tN9ettf8AP7jtdaDvd3XYp3VsLloVjWNQpUGJlK9zkAnnGeOOOfbNQyaVC7f6PJhQpZnYnb14A4/rWk0UkcXlptVCCGwOW+pqlIjxoU3NsJyVzxmpeRxnzOaT/wCB+V+vr3KhWv8ACyn/AGNdHOwKwDbe+emQfbP9ecVF/Zl4H2mAq3YMQM/TPWtGAMrLJI5SMAlSxwD7D8fyqnqGpyXZMYcmIE44wSOOD7cV83isJh8Pfnv5We7+a2/4Y6VKo3ZWZTeB0B3FOOoEik/z96fFaTTuqRBXZuih1z/P3/n6VBRXl3p822nr/wAA2s7blxNLunBIRQoz828Y6Z659qlTR5yzb2CqrbSw5455/Pj/AAp1hqrQKtvNkwknJXAbn37/AOfpU0kLqQ5IZWwQwORyM17GDwWGxM4qnfzu+vbT+vO5i5TV+bTsEOlwRF5LolohjaRnDevp9fpUtlatatO6lcsMKM4A69z9R+dOjgaZV3sSAMDnoKvJbu0axucqBj1JGeOa+gWRU6bTil9707+t+3/BOWeItuyg1x5sTrdoZFdlYLkgDGe/5U5LeOyLKA4SQZGANwIxwc9D+eO3etE2gxkcMOQR1FV47OWXEcjttRj164x2PpWuIwFNTU6SXb773fp/mRHEQa1dkPsIwylgpVONoJyen+Oa0NnGOKIIFhjCL0FTYrWpieWyT2PPqT55NopmxhYsSv3jkjPGaetuiD5R+PerNGKyjjXewnOb3ZjX8W0AnKp6gZOal0wGQSO4G4tzj17/AORVu7RTAwbGMdziqumQSJiVs7XXI+bpXTOpde1v0aOhT5qDXU0QiqSwAyeppGOBTiajY5OK+axFZzd2RTiNoopjtgY9a4m7HQld2GMctmkoorI3WgUUUUAFMkbC49afUDtuakyoq7G0UUUjUKKKKACiiigAooooAKKKKACiiigAooooAKeshXg8imUUA1csBgwyKWq4JByDUqyA8Hg07mbjbYfRRRTICiiigAooooAKKKKACjpRRQA8SetPDA9KhopqTIcEyeiow5HXmnhgelWmmQ4tC0UUUyQooooAAcU8NmmUVUZNCauS0oNRhvWn1tGfVGTj3HdaMD0ptLmuqGIa6mbgNkiSRdrDIyDSqiqMAUuaWuhYqTVrkuL2DA9KTApaKqNcnlGFaaV9KlpCO9dlKu+4tirIgx0+orKvAAQuQMnGa2pBkGsYQGS7O4jEeGYPyCM8/T6V3zxHJQcu+npc7MLq7voVNRP2ezSNTlZU656DIP8AQ8cVjVranC/k+YH3oSpCquQg5HXqOc8Vk18DmjbxHlZf18/v76nr0fhCiiivONQrctXa4sQ026WTPyr91vqP73Tr65z0FYdbumiO1sRO0JEjNtLbjkj9McH1569hXp5VOca14dv1RjX2Xcu2JDqMda01UYrMsWLSAKGEYACg9vWtZR0r7utWbgpNWPFxGk7ChaUKBS0V5NSu1oYJBRQTim5rhnV7lqI6kyKSisvasvlB1Rxh1DDOcEUDCqFUAADAA7UEgdaYWJ6Up4iXLytjUBWbsKZRRXI3c2SsITgZNRE5OaV23HjpTayk7m8Y2CiiikUFFFFADXbavuagpztub2FNqWaxVkFFFFBQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAD1kK8HkVKGDDIqvSgkHINFyXG5YopiyA8Hg0+qM2rBRRRQIKKKKACiiigAooooAKKKKAHhyOvNPDA1DRTUmS4Jk9FRhyOvNPDA9KtNMzcWhaKKKZIUoJFJRQBIGBpaipwYjrWsZ9yHHsPopAQaWtCBc0tNorSNRrclx7DqKKK66U9bGTRG/ArF1BQcdSfrWhqE7wqAEbaerDtWSyvIx3OrYIyH+X0/x9a9uNSNOi3Lt2v6HXhabupkV+62tutu8GQ4AZgwB6gkZx+XpjvyKyJYZIH2SKVbGcV0E02x0UwqU2hwN27rjufp09vTikfyr1AbnarkD5tvUBuOew655HT8/l8bgZVkqvN5LTTzXV6v5I9GE3Hp6nO0Vo3Wmx20SytcEqwO3Cg56e/qT+Xeq9jBFcTiOUvz/d9MEk59vpXiPC1FUjTe721XU3501zDba1kupNsY4HVsE4/Ktq0tprRyLmcOhXaEDEnHHH0w3b09qerWts4g8ph5aMu5RtJJ6nv2H+e0PktwWLNGvIB9Pz46etfRZdlUuX2tPVrrqr+S/r7jmqVObSWiJ7OcxErsOB8xx6H/APXW3EwZAw6EZFY0QD35aXJPXDKQRk9Dx79f/rVtqAFAHSvZrVG6Cc3r/wADr3Z5uKUeZWFpCaU02vEqT+8yigoophf0rnbS3NUrjiQKaWJ6U2isnNstRSCiiioKCo3fPAod88CmVEpGkY9WFFFFSaBRRRQAUyRtq8dTTzwM1XZtzE0mVFXYlFFFI1CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAp6yFeDyKZRQDVywGDDIpargkHINSrIDweDTuZuNth9FFFMgKKKKACiiigAooooAKKKKACiiigB6vjrzTwQRkVDQCR0qlIhwT2J6KYr9j+dPqk7mbTW4UUUUxBTw/rTKKak1sJpMloqMEingg1tGSZDjYWnU2itYT5TNq4rKGGCKp3lqJIjtXn2HNXaK9XD4qUNLmabi7ox4rFvvuoDEdB2qrdwCM4UcnnAroCtVbi3WTnnPqK9ajWjKHJFLyubwxL57yMe4tnurOKHOxs8Bug528475z2PGfSqlnYtHcO7SDMS7lZQcHpz09D25+lXZoHg2uu0MnQqOvPWluJN8Cp5mWAG5g3D89PXjGefevAxOWKFaE5rX/AINl92+h6Ual1aOzGsFe6V0bdvOcYPTvzgVsRwrtGRmqNlbDhjn1wa1kFe9Lmw9Pkvc87E1E2kuhAbKJ2RiuNhyMVaopDXjYrEyn8XQwV5biGiimMe1eVKVtWbJX0Bmz9KbRRWDdzVKwUUUhIAyaQC1Ez54HShmLfSm1DkaxjbVhRRRUlhRRRQAUUU122r70AtRkjZ+UfjUdFFSbJWQUUUUDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAHrIV4PIqUEEZFV6UEg5FFyXG5YopiyBuDwafVGbVgooooEFFFFABRRRQAUUUUAFFFFABTlYr9KbRQDVyZWDUtQVIr9jVqXcylDsPoooqiAo6UUUAPVs/WnVFUinI961hK+jM5RtqOBpabSiumnLoZSXUWo26VJUEsqpkEjPXFexhG5NWMra6GffSKoIJ5OeKoBMLskJUZ3BsZA7f5+lT3Ewe6G0huMbW6Hn/ACfwq9BaZi/e/Mx+9716VZOcuVuyj163/wCAehGaowTfUq2tyYm2SjHvnitaGRZFDKwIPcVny6WHclSoXH3cf4U2FZbO6CFQ5cHG3j36dqid6qcZWfn/AMAxqRp1FeD17GvTaELGNS67WIGVznB9KOlfNV371jKCEY4FR0pOTmkrhlK7OiKsFFFMZ8cDrUt2KSuOZgtREknmk60Vm3c1jGwUUUUigooooAKKKKAEJwMmoGbcc0533HjpTKlmsY2CiiigoKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAp6yFeDyKZRQDVywCCMilquCQcg1KsgPB4NO5m42H0UUUyAooooAKKKKACiiigAooooAKKKKAHKxX6VICCMioaUEg5FNSsTKNyaimqwb606tDJqwUoODSUUCJQcigdaYp7U+uiEr6mMlbQdWVqTRh0ViNx7FcnH8/5Vqk4FZN7IklxGAcqh3OR2Fe/gablCS6CofxLkNla+bIyTBmUDIJXYD9AOnb8q2woApsYAUYpxNZYmuqcVShsiak3Vldi1FJCJGjbO0o2en6U/NISBXAsZOGsQUBxNRs2fpSFiaSuCpUcmbRjYKKCcDJqJn3cDpWTdjVRuKz9hTKKKzbuapWCiiigYUUUUAFFFFABUUj54HSh3zwOlR0maRj1YUUUUiwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAHK5X3HpUysG6VXoouS4plmioll/vfnUgIIyDTM2mhaKKKYgooooAKKKKACiiigAooooAKer9j+dMooTsJpMnoqEMV6VIHB+taKVzJxaHU9Wz9aZRVxlYhq4s6PKm1XUA9QRyfx7U5IIo02KihfQCkDmjf7V6CzCagoLSxk6b2HIqxoFXOB6mgsBTCxNJXJVryqS5nuUoWHFiabRRWLbe5olYKQsFHNNZ+w/OoycnJqHIuML7isxY0lFFQahRRRQAUUUUAFFFBIAyaAConfPA6Ujvu4HSmUrmkY9WFFFFIsKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAClBK9DSUUASrKP4uKkqtSqxXoadyHDsWKKjWUHrxUmc9KCGmgooopiCiiigAooooAKKKKACiiigBwcjvThIO/FR0U02iXFMm3Ke9LUFFPmJ9mT9KQso71DRRzD9mSGT0FMLE9aSik22UopBRRRSGFFFFABRRRQAUUZx1qJpey/nSGk2PZwvXr6VEzFj7elNopGiikFFFFBQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFKCV6GiigCRZf7w/KnghuhoopkSirXFooopmYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUxpAOnNFFJlxSZEzFjzSUUUjQKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries for simulation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Imports for visualization\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def DisplayFractal(a, fmt='jpeg'):\n",
    "  \"\"\"Display an array of iteration counts as a\n",
    "     colorful picture of a fractal.\"\"\"\n",
    "  a_cyclic = (6.28*a/20.0).reshape(list(a.shape)+[1])\n",
    "  img = np.concatenate([10+20*np.cos(a_cyclic),\n",
    "                        30+50*np.sin(a_cyclic),\n",
    "                        155-80*np.cos(a_cyclic)], 2)\n",
    "  img[a==a.max()] = 0\n",
    "  a = img\n",
    "  a = np.uint8(np.clip(a, 0, 255))\n",
    "  f = BytesIO()\n",
    "  PIL.Image.fromarray(a).save(f, fmt)\n",
    "  display(Image(data=f.getvalue()))\n",
    "\n",
    "# Use NumPy to create a 2D array of complex numbers\n",
    "\n",
    "Y, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005]\n",
    "Z = X+1j*Y\n",
    "\n",
    "xs = tf.constant(Z.astype(np.complex64))\n",
    "zs = tf.Variable(xs)\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# Operation to update the zs and the iteration count.\n",
    "#\n",
    "# Note: We keep computing zs after they diverge! This\n",
    "#       is very wasteful! There are better, if a little\n",
    "#       less simple, ways to do this.\n",
    "#\n",
    "for i in range(20000):\n",
    "    # Compute the new values of z: z^2 + x\n",
    "    zs_ = zs*zs + xs\n",
    "\n",
    "    # Have we diverged with this new value?\n",
    "    not_diverged = tf.abs(zs_) < 4\n",
    "\n",
    "    zs.assign(zs_),\n",
    "    ns.assign_add(tf.cast(not_diverged, tf.float32))\n",
    "    \n",
    "DisplayFractal(ns.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mandlebrot render is both simple and infinitely complex at the same time. This view shows the entire Mandlebrot universe at the same time, as it is completely zoomed out. However, if you zoom in on any non-black portion of the plot, you will find infinite hidden complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a layer on top of Tensorflow that makes it much easier to create neural networks.  Rather than define the graphs, as you see above, you set the individual layers of the network with a much more high-level API.  Unless you are performing research into entirely new structures of deep neural networks, it is unlikely that you need to program TensorFlow directly.  \n",
    "\n",
    "**For this class, we will use usually use TensorFlow through Keras, rather than direct TensorFlow**\n",
    "\n",
    "### Simple TensorFlow Regression: MPG\n",
    "\n",
    "This example shows how to encode the MPG dataset for regression.  This dataset is slightly more complicated than Iris, because:\n",
    "\n",
    "* Input has both numeric and categorical\n",
    "* Input has missing values\n",
    "\n",
    "This example uses functions defined above in this notepad, the \"helpful functions\". These functions allow you to build the feature vector for a neural network. Consider the following:\n",
    "\n",
    "* Predictors/Inputs \n",
    "    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n",
    "    * Encode textual/categorical values with **encode_text_dummy**.\n",
    "    * Encode numeric values with **encode_numeric_zscore**.\n",
    "* Output\n",
    "    * Discard rows with missing outputs.\n",
    "    * Encode textual/categorical values with **encode_text_index**.\n",
    "    * Do not encode output numeric values.\n",
    "* Produce final feature vectors (x) and expected output (y) with **to_xy**.\n",
    "\n",
    "To encode categorical values that are part of the feature vector, use the functions from above if the categorical value is the target (as was the case with Iris, use the same technique as Iris). The iris technique allows you to decode back to Iris text strings from the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples\n",
      "Epoch 1/100\n",
      "398/398 - 0s - loss: 532928.3769\n",
      "Epoch 2/100\n",
      "398/398 - 0s - loss: 253903.1740\n",
      "Epoch 3/100\n",
      "398/398 - 0s - loss: 100463.9936\n",
      "Epoch 4/100\n",
      "398/398 - 0s - loss: 39376.4139\n",
      "Epoch 5/100\n",
      "398/398 - 0s - loss: 14699.5505\n",
      "Epoch 6/100\n",
      "398/398 - 0s - loss: 10070.2764\n",
      "Epoch 7/100\n",
      "398/398 - 0s - loss: 8767.4534\n",
      "Epoch 8/100\n",
      "398/398 - 0s - loss: 7568.0651\n",
      "Epoch 9/100\n",
      "398/398 - 0s - loss: 6519.3935\n",
      "Epoch 10/100\n",
      "398/398 - 0s - loss: 5638.3222\n",
      "Epoch 11/100\n",
      "398/398 - 0s - loss: 4900.6222\n",
      "Epoch 12/100\n",
      "398/398 - 0s - loss: 4256.2369\n",
      "Epoch 13/100\n",
      "398/398 - 0s - loss: 3715.3158\n",
      "Epoch 14/100\n",
      "398/398 - 0s - loss: 3243.3127\n",
      "Epoch 15/100\n",
      "398/398 - 0s - loss: 2837.4957\n",
      "Epoch 16/100\n",
      "398/398 - 0s - loss: 2493.2320\n",
      "Epoch 17/100\n",
      "398/398 - 0s - loss: 2191.6368\n",
      "Epoch 18/100\n",
      "398/398 - 0s - loss: 1935.9826\n",
      "Epoch 19/100\n",
      "398/398 - 0s - loss: 1709.9219\n",
      "Epoch 20/100\n",
      "398/398 - 0s - loss: 1517.5594\n",
      "Epoch 21/100\n",
      "398/398 - 0s - loss: 1347.0029\n",
      "Epoch 22/100\n",
      "398/398 - 0s - loss: 1202.3976\n",
      "Epoch 23/100\n",
      "398/398 - 0s - loss: 1074.8017\n",
      "Epoch 24/100\n",
      "398/398 - 0s - loss: 965.3571\n",
      "Epoch 25/100\n",
      "398/398 - 0s - loss: 869.1089\n",
      "Epoch 26/100\n",
      "398/398 - 0s - loss: 786.9213\n",
      "Epoch 27/100\n",
      "398/398 - 0s - loss: 712.5439\n",
      "Epoch 28/100\n",
      "398/398 - 0s - loss: 648.9730\n",
      "Epoch 29/100\n",
      "398/398 - 0s - loss: 592.6204\n",
      "Epoch 30/100\n",
      "398/398 - 0s - loss: 542.2534\n",
      "Epoch 31/100\n",
      "398/398 - 0s - loss: 499.5945\n",
      "Epoch 32/100\n",
      "398/398 - 0s - loss: 460.6432\n",
      "Epoch 33/100\n",
      "398/398 - 0s - loss: 426.6404\n",
      "Epoch 34/100\n",
      "398/398 - 0s - loss: 397.0015\n",
      "Epoch 35/100\n",
      "398/398 - 0s - loss: 371.2881\n",
      "Epoch 36/100\n",
      "398/398 - 0s - loss: 347.1069\n",
      "Epoch 37/100\n",
      "398/398 - 0s - loss: 327.0558\n",
      "Epoch 38/100\n",
      "398/398 - 0s - loss: 308.9706\n",
      "Epoch 39/100\n",
      "398/398 - 0s - loss: 293.4671\n",
      "Epoch 40/100\n",
      "398/398 - 0s - loss: 279.4684\n",
      "Epoch 41/100\n",
      "398/398 - 0s - loss: 266.5202\n",
      "Epoch 42/100\n",
      "398/398 - 0s - loss: 255.4313\n",
      "Epoch 43/100\n",
      "398/398 - 0s - loss: 245.8519\n",
      "Epoch 44/100\n",
      "398/398 - 0s - loss: 237.3705\n",
      "Epoch 45/100\n",
      "398/398 - 0s - loss: 229.9772\n",
      "Epoch 46/100\n",
      "398/398 - 0s - loss: 223.4570\n",
      "Epoch 47/100\n",
      "398/398 - 0s - loss: 217.3037\n",
      "Epoch 48/100\n",
      "398/398 - 0s - loss: 212.0329\n",
      "Epoch 49/100\n",
      "398/398 - 0s - loss: 207.3768\n",
      "Epoch 50/100\n",
      "398/398 - 0s - loss: 203.4740\n",
      "Epoch 51/100\n",
      "398/398 - 0s - loss: 199.8031\n",
      "Epoch 52/100\n",
      "398/398 - 0s - loss: 196.9008\n",
      "Epoch 53/100\n",
      "398/398 - 0s - loss: 194.1096\n",
      "Epoch 54/100\n",
      "398/398 - 0s - loss: 191.8634\n",
      "Epoch 55/100\n",
      "398/398 - 0s - loss: 189.6037\n",
      "Epoch 56/100\n",
      "398/398 - 0s - loss: 187.9038\n",
      "Epoch 57/100\n",
      "398/398 - 0s - loss: 186.4501\n",
      "Epoch 58/100\n",
      "398/398 - 0s - loss: 184.9448\n",
      "Epoch 59/100\n",
      "398/398 - 0s - loss: 183.7436\n",
      "Epoch 60/100\n",
      "398/398 - 0s - loss: 182.6323\n",
      "Epoch 61/100\n",
      "398/398 - 0s - loss: 181.8248\n",
      "Epoch 62/100\n",
      "398/398 - 0s - loss: 180.8635\n",
      "Epoch 63/100\n",
      "398/398 - 0s - loss: 180.1527\n",
      "Epoch 64/100\n",
      "398/398 - 0s - loss: 179.4130\n",
      "Epoch 65/100\n",
      "398/398 - 0s - loss: 178.8714\n",
      "Epoch 66/100\n",
      "398/398 - 0s - loss: 178.4050\n",
      "Epoch 67/100\n",
      "398/398 - 0s - loss: 177.8955\n",
      "Epoch 68/100\n",
      "398/398 - 0s - loss: 177.4985\n",
      "Epoch 69/100\n",
      "398/398 - 0s - loss: 177.1324\n",
      "Epoch 70/100\n",
      "398/398 - 0s - loss: 176.8686\n",
      "Epoch 71/100\n",
      "398/398 - 0s - loss: 176.5298\n",
      "Epoch 72/100\n",
      "398/398 - 0s - loss: 176.3295\n",
      "Epoch 73/100\n",
      "398/398 - 0s - loss: 176.0535\n",
      "Epoch 74/100\n",
      "398/398 - 0s - loss: 175.8712\n",
      "Epoch 75/100\n",
      "398/398 - 0s - loss: 175.6723\n",
      "Epoch 76/100\n",
      "398/398 - 0s - loss: 175.5422\n",
      "Epoch 77/100\n",
      "398/398 - 0s - loss: 175.3606\n",
      "Epoch 78/100\n",
      "398/398 - 0s - loss: 175.2149\n",
      "Epoch 79/100\n",
      "398/398 - 0s - loss: 175.1186\n",
      "Epoch 80/100\n",
      "398/398 - 0s - loss: 174.9576\n",
      "Epoch 81/100\n",
      "398/398 - 0s - loss: 174.8516\n",
      "Epoch 82/100\n",
      "398/398 - 0s - loss: 174.7489\n",
      "Epoch 83/100\n",
      "398/398 - 0s - loss: 174.6472\n",
      "Epoch 84/100\n",
      "398/398 - 0s - loss: 174.5678\n",
      "Epoch 85/100\n",
      "398/398 - 0s - loss: 174.4445\n",
      "Epoch 86/100\n",
      "398/398 - 0s - loss: 174.3567\n",
      "Epoch 87/100\n",
      "398/398 - 0s - loss: 174.2643\n",
      "Epoch 88/100\n",
      "398/398 - 0s - loss: 174.1730\n",
      "Epoch 89/100\n",
      "398/398 - 0s - loss: 174.1209\n",
      "Epoch 90/100\n",
      "398/398 - 0s - loss: 174.0430\n",
      "Epoch 91/100\n",
      "398/398 - 0s - loss: 173.9585\n",
      "Epoch 92/100\n",
      "398/398 - 0s - loss: 173.8340\n",
      "Epoch 93/100\n",
      "398/398 - 0s - loss: 173.7509\n",
      "Epoch 94/100\n",
      "398/398 - 0s - loss: 173.6775\n",
      "Epoch 95/100\n",
      "398/398 - 0s - loss: 173.5709\n",
      "Epoch 96/100\n",
      "398/398 - 0s - loss: 173.5111\n",
      "Epoch 97/100\n",
      "398/398 - 0s - loss: 173.4251\n",
      "Epoch 98/100\n",
      "398/398 - 0s - loss: 173.3334\n",
      "Epoch 99/100\n",
      "398/398 - 0s - loss: 173.2709\n",
      "Epoch 100/100\n",
      "398/398 - 0s - loss: 173.1735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x275f09bc408>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "cars = df['name']\n",
    "\n",
    "# Handle missing value\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "# Pandas to Numpy\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values # regression\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Neural Network Hyperparameters\n",
    "\n",
    "If you look at the above code, you will see that the neural network contains four layers.  The first layer is the input layer because it contains the **input_dim** parameter that the programmer sets to be the number of inputs that the dataset has.  The network needs one input neuron for every column in the data set (including dummy variables).  \n",
    "\n",
    "There are also several hidden layers, with 25 and 10 neurons each. You might be wondering how the programmer chose these numbers. Selecting a hidden neuron structure is one of the most common questions about neural networks.  Unfortunately, there is not a right answer.  These are hyperparameters.  They are settings that can affect neural network performance, yet there are not a clearly defined means of setting them.\n",
    "\n",
    "In general, more hidden neurons mean more capability to fit complex problems.  However, too many neurons can lead to overfitting and lengthy training times.  Too few can lead to underfitting the problem and will sacrifice accuracy.  Also, how many layers you have is another hyperparameter.  In general, more layers allow the neural network to be able to perform more of its feature engineering and data preprocessing.  But this also comes at the expense of training times and the risk of overfitting.  In general, you will see that neuron counts start larger near the input layer and tend to shrink towards the output layer in a sort of triangular fashion. \n",
    "\n",
    "Some techniques use machine learning to optimize these values.  These will be discussed in [Module 8.3](t81_558_class_08_3_keras_hyperparameters.ipynb).\n",
    "\n",
    "### Controlling the Amount of Output\n",
    "\n",
    "The program produces one line of output for each training epoch.  You can eliminate this output by setting the verbose setting of the fit command:\n",
    "\n",
    "* **verbose=0** - No progress output (use with Jupyter if you do not want output)\n",
    "* **verbose=1** - Display progress bar, does not work well with Jupyter\n",
    "* **verbose=2** - Summary progress output (use with Jupyter if you want to know the loss at each epoch)\n",
    "\n",
    "### Regression Prediction\n",
    "\n",
    "Next, we will perform actual predictions.  The program assigns these predictions to the **pred** variable. These are all MPG predictions from the neural network.  Notice that this is a 2D array?  You can always see the dimensions of what Keras returns by printing out **pred.shape**.  Neural networks can return multiple values, so the result is always an array.  Here the neural network only returns one value per prediction (there are 398 cars, so 398 predictions).  However, a 2D range is needed because the neural network has the potential of returning more than one value.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (398, 1)\n",
      "[[23.08987 ]\n",
      " [23.776747]\n",
      " [22.190557]\n",
      " [22.24273 ]\n",
      " [22.536469]\n",
      " [27.865118]\n",
      " [27.486967]\n",
      " [27.316473]\n",
      " [27.953611]\n",
      " [24.391483]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x)\n",
    "print(f\"Shape: {pred.shape}\")\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to see how good these predictions are.  We know what the correct MPG is for each car, so we can measure how close the neural network was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 13.157274080377979\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "print(f\"Final score (RMSE): {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number printed above is the average amount that the predictions were above or below the expected output. We can also print out the first ten cars, with predictions and actual MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Car name: chevrolet chevelle malibu, MPG: 18.0, predicted MPG: {pred[i]}\n",
      "2. Car name: buick skylark 320, MPG: 15.0, predicted MPG: {pred[i]}\n",
      "3. Car name: plymouth satellite, MPG: 18.0, predicted MPG: {pred[i]}\n",
      "4. Car name: amc rebel sst, MPG: 16.0, predicted MPG: {pred[i]}\n",
      "5. Car name: ford torino, MPG: 17.0, predicted MPG: {pred[i]}\n",
      "6. Car name: ford galaxie 500, MPG: 15.0, predicted MPG: {pred[i]}\n",
      "7. Car name: chevrolet impala, MPG: 14.0, predicted MPG: {pred[i]}\n",
      "8. Car name: plymouth fury iii, MPG: 14.0, predicted MPG: {pred[i]}\n",
      "9. Car name: pontiac catalina, MPG: 14.0, predicted MPG: {pred[i]}\n",
      "10. Car name: amc ambassador dpl, MPG: 15.0, predicted MPG: {pred[i]}\n"
     ]
    }
   ],
   "source": [
    "# Sample predictions\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}. Car name: {cars[i]}, MPG: {y[i]}, \" \n",
    "          + \"predicted MPG: {pred[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple TensorFlow Classification: Iris\n",
    "\n",
    "Classification is the process by which a neural network attempts to classify the input into one or more classes.  The simplest way of evaluating a classification network is to track the percentage of training set items that were classified incorrectly.  We typically score human results in this manner. For example, you might have taken multiple-choice exams in school in which you had to shade in a bubble for choices A, B, C, or D.  If you chose the wrong letter on a 10-question exam, you would earn a 90%.  In the same way, we can grade computers; however, most classification algorithms do not merely choose A, B, C, or D.  Computers typically report a classification as their percent confidence in each class.  Figure 3.EXAM shows how a computer and a human might both respond to question number 1 on an exam.\n",
    "\n",
    "**Figure 3.EXAM: Classification Neural Network Output**\n",
    "![Classification Neural Network Output](images/class-multi-choice.png \"Classification Neural Network Output\")\n",
    "\n",
    "As you can see, the human test taker marked the first question as \"B.\" However, the computer test taker had an 80% (0.8) confidence in \"B\" and was also somewhat sure with 10% (0.1) on \"A.\" The computer then distributed the remaining points on the other two.  In the simplest sense, the machine would get 80% of the score for this question if the correct answer were \"B.\" The computer would get only 5% (0.05) of the points if the correct answer were \"D.\" \n",
    "\n",
    "What we just saw is a straightforward example of how to perform the Iris classification using TensorFlow.  The iris.csv file is used, rather than using the built-in data that many of the Google examples require.  \n",
    "\n",
    "**Make sure that you always run previous code blocks.  If you run the code block below, without the code block above, you will get errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/100\n",
      "150/150 - 0s - loss: 1.1343\n",
      "Epoch 2/100\n",
      "150/150 - 0s - loss: 1.0067\n",
      "Epoch 3/100\n",
      "150/150 - 0s - loss: 0.9150\n",
      "Epoch 4/100\n",
      "150/150 - 0s - loss: 0.8430\n",
      "Epoch 5/100\n",
      "150/150 - 0s - loss: 0.7892\n",
      "Epoch 6/100\n",
      "150/150 - 0s - loss: 0.7348\n",
      "Epoch 7/100\n",
      "150/150 - 0s - loss: 0.6847\n",
      "Epoch 8/100\n",
      "150/150 - 0s - loss: 0.6382\n",
      "Epoch 9/100\n",
      "150/150 - 0s - loss: 0.6041\n",
      "Epoch 10/100\n",
      "150/150 - 0s - loss: 0.5686\n",
      "Epoch 11/100\n",
      "150/150 - 0s - loss: 0.5374\n",
      "Epoch 12/100\n",
      "150/150 - 0s - loss: 0.5166\n",
      "Epoch 13/100\n",
      "150/150 - 0s - loss: 0.4913\n",
      "Epoch 14/100\n",
      "150/150 - 0s - loss: 0.4706\n",
      "Epoch 15/100\n",
      "150/150 - 0s - loss: 0.4519\n",
      "Epoch 16/100\n",
      "150/150 - 0s - loss: 0.4337\n",
      "Epoch 17/100\n",
      "150/150 - 0s - loss: 0.4207\n",
      "Epoch 18/100\n",
      "150/150 - 0s - loss: 0.4026\n",
      "Epoch 19/100\n",
      "150/150 - 0s - loss: 0.3894\n",
      "Epoch 20/100\n",
      "150/150 - 0s - loss: 0.3764\n",
      "Epoch 21/100\n",
      "150/150 - 0s - loss: 0.3630\n",
      "Epoch 22/100\n",
      "150/150 - 0s - loss: 0.3527\n",
      "Epoch 23/100\n",
      "150/150 - 0s - loss: 0.3409\n",
      "Epoch 24/100\n",
      "150/150 - 0s - loss: 0.3296\n",
      "Epoch 25/100\n",
      "150/150 - 0s - loss: 0.3179\n",
      "Epoch 26/100\n",
      "150/150 - 0s - loss: 0.3091\n",
      "Epoch 27/100\n",
      "150/150 - 0s - loss: 0.2985\n",
      "Epoch 28/100\n",
      "150/150 - 0s - loss: 0.2884\n",
      "Epoch 29/100\n",
      "150/150 - 0s - loss: 0.2825\n",
      "Epoch 30/100\n",
      "150/150 - 0s - loss: 0.2703\n",
      "Epoch 31/100\n",
      "150/150 - 0s - loss: 0.2643\n",
      "Epoch 32/100\n",
      "150/150 - 0s - loss: 0.2554\n",
      "Epoch 33/100\n",
      "150/150 - 0s - loss: 0.2482\n",
      "Epoch 34/100\n",
      "150/150 - 0s - loss: 0.2395\n",
      "Epoch 35/100\n",
      "150/150 - 0s - loss: 0.2341\n",
      "Epoch 36/100\n",
      "150/150 - 0s - loss: 0.2358\n",
      "Epoch 37/100\n",
      "150/150 - 0s - loss: 0.2174\n",
      "Epoch 38/100\n",
      "150/150 - 0s - loss: 0.2181\n",
      "Epoch 39/100\n",
      "150/150 - 0s - loss: 0.2091\n",
      "Epoch 40/100\n",
      "150/150 - 0s - loss: 0.2036\n",
      "Epoch 41/100\n",
      "150/150 - 0s - loss: 0.1967\n",
      "Epoch 42/100\n",
      "150/150 - 0s - loss: 0.1946\n",
      "Epoch 43/100\n",
      "150/150 - 0s - loss: 0.1917\n",
      "Epoch 44/100\n",
      "150/150 - 0s - loss: 0.1816\n",
      "Epoch 45/100\n",
      "150/150 - 0s - loss: 0.1884\n",
      "Epoch 46/100\n",
      "150/150 - 0s - loss: 0.1736\n",
      "Epoch 47/100\n",
      "150/150 - 0s - loss: 0.1745\n",
      "Epoch 48/100\n",
      "150/150 - 0s - loss: 0.1655\n",
      "Epoch 49/100\n",
      "150/150 - 0s - loss: 0.1620\n",
      "Epoch 50/100\n",
      "150/150 - 0s - loss: 0.1579\n",
      "Epoch 51/100\n",
      "150/150 - 0s - loss: 0.1552\n",
      "Epoch 52/100\n",
      "150/150 - 0s - loss: 0.1511\n",
      "Epoch 53/100\n",
      "150/150 - 0s - loss: 0.1493\n",
      "Epoch 54/100\n",
      "150/150 - 0s - loss: 0.1457\n",
      "Epoch 55/100\n",
      "150/150 - 0s - loss: 0.1434\n",
      "Epoch 56/100\n",
      "150/150 - 0s - loss: 0.1426\n",
      "Epoch 57/100\n",
      "150/150 - 0s - loss: 0.1396\n",
      "Epoch 58/100\n",
      "150/150 - 0s - loss: 0.1402\n",
      "Epoch 59/100\n",
      "150/150 - 0s - loss: 0.1320\n",
      "Epoch 60/100\n",
      "150/150 - 0s - loss: 0.1310\n",
      "Epoch 61/100\n",
      "150/150 - 0s - loss: 0.1279\n",
      "Epoch 62/100\n",
      "150/150 - 0s - loss: 0.1290\n",
      "Epoch 63/100\n",
      "150/150 - 0s - loss: 0.1237\n",
      "Epoch 64/100\n",
      "150/150 - 0s - loss: 0.1224\n",
      "Epoch 65/100\n",
      "150/150 - 0s - loss: 0.1197\n",
      "Epoch 66/100\n",
      "150/150 - 0s - loss: 0.1192\n",
      "Epoch 67/100\n",
      "150/150 - 0s - loss: 0.1155\n",
      "Epoch 68/100\n",
      "150/150 - 0s - loss: 0.1175\n",
      "Epoch 69/100\n",
      "150/150 - 0s - loss: 0.1135\n",
      "Epoch 70/100\n",
      "150/150 - 0s - loss: 0.1146\n",
      "Epoch 71/100\n",
      "150/150 - 0s - loss: 0.1092\n",
      "Epoch 72/100\n",
      "150/150 - 0s - loss: 0.1090\n",
      "Epoch 73/100\n",
      "150/150 - 0s - loss: 0.1076\n",
      "Epoch 74/100\n",
      "150/150 - 0s - loss: 0.1044\n",
      "Epoch 75/100\n",
      "150/150 - 0s - loss: 0.1061\n",
      "Epoch 76/100\n",
      "150/150 - 0s - loss: 0.1021\n",
      "Epoch 77/100\n",
      "150/150 - 0s - loss: 0.1029\n",
      "Epoch 78/100\n",
      "150/150 - 0s - loss: 0.1022\n",
      "Epoch 79/100\n",
      "150/150 - 0s - loss: 0.0997\n",
      "Epoch 80/100\n",
      "150/150 - 0s - loss: 0.0989\n",
      "Epoch 81/100\n",
      "150/150 - 0s - loss: 0.0977\n",
      "Epoch 82/100\n",
      "150/150 - 0s - loss: 0.1010\n",
      "Epoch 83/100\n",
      "150/150 - 0s - loss: 0.0966\n",
      "Epoch 84/100\n",
      "150/150 - 0s - loss: 0.0953\n",
      "Epoch 85/100\n",
      "150/150 - 0s - loss: 0.0937\n",
      "Epoch 86/100\n",
      "150/150 - 0s - loss: 0.0965\n",
      "Epoch 87/100\n",
      "150/150 - 0s - loss: 0.0918\n",
      "Epoch 88/100\n",
      "150/150 - 0s - loss: 0.0923\n",
      "Epoch 89/100\n",
      "150/150 - 0s - loss: 0.0900\n",
      "Epoch 90/100\n",
      "150/150 - 0s - loss: 0.0897\n",
      "Epoch 91/100\n",
      "150/150 - 0s - loss: 0.0913\n",
      "Epoch 92/100\n",
      "150/150 - 0s - loss: 0.0871\n",
      "Epoch 93/100\n",
      "150/150 - 0s - loss: 0.0900\n",
      "Epoch 94/100\n",
      "150/150 - 0s - loss: 0.0882\n",
      "Epoch 95/100\n",
      "150/150 - 0s - loss: 0.0867\n",
      "Epoch 96/100\n",
      "150/150 - 0s - loss: 0.0854\n",
      "Epoch 97/100\n",
      "150/150 - 0s - loss: 0.0847\n",
      "Epoch 98/100\n",
      "150/150 - 0s - loss: 0.0840\n",
      "Epoch 99/100\n",
      "150/150 - 0s - loss: 0.0852\n",
      "Epoch 100/100\n",
      "150/150 - 0s - loss: 0.0871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x275f1d9bdc8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "dummies = pd.get_dummies(df['species']) # Classification\n",
    "species = dummies.columns\n",
    "y = dummies.values\n",
    "\n",
    "\n",
    "# Build neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print out number of species found:\n",
    "\n",
    "print(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a neural network training, we would like to be able to use it. The following code makes use of our neural network. Exactly like before, we will generate predictions.  Notice that three values come back for each of the 150 iris flowers.  There were three types of iris (Iris-setosa, Iris-versicolor, and Iris-virginica).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (150, 3)\n",
      "[[9.98257935e-01 1.74211117e-03 1.47250105e-08]\n",
      " [9.94762123e-01 5.23775769e-03 9.35751956e-08]\n",
      " [9.97034669e-01 2.96533993e-03 5.68744980e-08]\n",
      " [9.94459629e-01 5.54029271e-03 1.55113412e-07]\n",
      " [9.98531222e-01 1.46873493e-03 1.33291875e-08]\n",
      " [9.98098075e-01 1.90198515e-03 1.37531018e-08]\n",
      " [9.96991158e-01 3.00874189e-03 7.64488419e-08]\n",
      " [9.97346044e-01 2.65395525e-03 3.05425694e-08]\n",
      " [9.92665589e-01 7.33401440e-03 3.12981911e-07]\n",
      " [9.95905280e-01 4.09475202e-03 5.66798697e-08]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x)\n",
    "print(f\"Shape: {pred.shape}\")\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to turn of scientific notation, the following line can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see these values rounded up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the program considers the column with the highest prediction to be the prediction of the neural network.  It is easy to convert the predictions to the expected iris species.  The argmax function finds the index of the maximum prediction for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Expected: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y,axis=1)\n",
    "print(f\"Predictions: {predict_classes}\")\n",
    "print(f\"Expected: {expected_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it is straightforward to turn these indexes back into iris species. We use the species list that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
      "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
      "       'Iris-setosa'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(species[predict_classes[1:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy might be a more easily understood error metric.  It is essentially a test score.  For all of the iris predictions, what percent were correct?  The downside is it does not consider how confident the neural network was in each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs two ad hoc predictions.  The first prediction is simply a single iris flower, and the second predicts two iris flowers.  Notice that the argmax in the second prediction requires **axis=1**?  Since we have a 2D array now, we must specify which axis to take the argmax over.  The value **axis=1** specifies we want the max column index for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00208851 0.19842853 0.799483  ]]\n",
      "Predict that [[5. 3. 4. 2.]] is: Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "sample_flower = np.array( [[5.0,3.0,4.0,2.0]], dtype=float)\n",
    "pred = model.predict(sample_flower)\n",
    "print(pred)\n",
    "pred = np.argmax(pred)\n",
    "print(f\"Predict that {sample_flower} is: {species[pred]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also predict two sample flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00208851 0.19842838 0.79948306]\n",
      " [0.9900221  0.00997756 0.00000035]]\n",
      "Predict that these two flowers [[5.  3.  4.  2. ]\n",
      " [5.2 3.5 1.5 0.8]] \n",
      "are: Index(['Iris-virginica', 'Iris-setosa'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sample_flower = np.array( [[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]],\\\n",
    "        dtype=float)\n",
    "pred = model.predict(sample_flower)\n",
    "print(pred)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(f\"Predict that these two flowers {sample_flower} \")\n",
    "print(f\"are: {species[pred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
